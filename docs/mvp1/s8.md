Sprint 8 Backlog: Testing & Documentation
Sprint Goal: To ensure the application is production-ready by conducting comprehensive, end-to-end system testing, creating complete user and technical documentation, and preparing robust deployment packages and procedures for various operating environments.

Introduction
This document provides an atomic breakdown of the tasks for Sprint 8, the final phase of the MVP development cycle. With all functional components and performance optimizations now in place, this sprint is entirely focused on quality assurance, reliability, and knowledge transfer. The work involves a multi-layered testing strategy to validate every aspect of the application, the creation of professional documentation for all user personas (end-users, administrators, developers), and the packaging of the software for a seamless deployment experience.

User Stories
User Story 1: Comprehensive System Testing and Quality Assurance

Description: As a Quality Assurance Manager, I need to execute a complete and exhaustive testing plan that covers the entire application. This is to guarantee that the software is stable, reliable, and functionally correct, ensuring it meets the professional quality standards required for a successful production release and earning the trust of our users.

Extremely Atomic Actions to Undertake:

Develop End-to-End Integration Test Suite:

Create a new test file: tests/integration/end_to_end_testing.cpp.

Inside this file, write a test case named FullRegistrationWorkflowTest.

Step 1: Programmatically simulate creating a new project.

Step 2: Simulate importing two different, but overlapping, scan files (e.g., scan_part_A.e57, scan_part_B.e57).

Step 3: Simulate creating three corresponding NaturalPointTarget pairs between the two scans.

Step 4: Simulate running the manual alignment and assert that the resulting RMS error is below a predefined threshold.

Step 5: Simulate running the ICP refinement algorithm.

Step 6: Assert that the final RMS error after ICP is lower than the manual alignment error.

Step 7: Simulate exporting the final aligned point cloud to a .las file.

Step 8: Write a separate utility to read the exported .las file and verify its integrity and point count.

Execute Stress and Boundary Testing:

Stress Test: Create a test script that repeatedly loads and unloads a 50-million-point dataset for one hour. Monitor memory usage throughout. Assert that memory usage does not continuously grow (indicating a leak).

Boundary Test: Write a unit test for the SphereDetector that passes it a point cloud containing spheres with radii exactly at, just below, and just above the configured detection limits. Assert that only the sphere with the radius exactly at the limit is detected.

Boundary Test: Write a unit test for ICPRegistration that provides an initial alignment that is rotated 179 degrees from the correct solution. Assert that the algorithm still converges correctly without getting stuck in a local minimum.

Formalize User Acceptance Testing (UAT):

Create a new document: testing/UAT_Plan.md.

In this document, write down a detailed script for a non-developer to follow.

Scenario 1 (Surveyor): "Using the provided Project_X dataset, register Scan_1 and Scan_2 using at least 5 checkerboard targets. The final RMS error must be below 2.0mm. Export the result as an E57 file."

Scenario 2 (GIS Specialist): "Load the aligned point cloud from Scenario 1. Export the data, transforming it from its source coordinate system (UTM 10N) to a target system (WGS84). Verify the output coordinates are correct."

Enhance CI/CD Pipeline:

Modify the project's CI/CD configuration file (e.g., .gitlab-ci.yml, azure-pipelines.yml).

Add a new "Performance_Regression" stage that runs after the unit tests.

This stage will execute a script that runs a benchmark test (e.g., timing the ICP alignment on a standard 10M point dataset).

The script must compare the execution time to a stored baseline value. If the new time is more than 10% slower than the baseline, the pipeline must fail.

References between Files:

The end_to_end_testing.cpp suite will depend on and interact with nearly all major classes: ProjectManager, ScanImportManager, AlignmentEngine, ICPRegistration, and PointCloudExporter.

The UAT plan (UAT_Plan.md) will reference specific test data files that must be prepared.

Acceptance Criteria:

The end-to-end integration test suite must pass with 100% success.

The 1-hour memory stress test must complete with no increase in baseline memory usage.

All UAT scenarios must be successfully completed by a non-developer tester, with all functional requirements met.

The CI/CD pipeline correctly fails a build if a code change introduces a performance regression of 15% or more.

Testing Plan:

Test Case 1.1: Full Workflow Test Execution.

Test Data: A prepared dataset with two overlapping scans (UAT_Project_Scans).

Expected Result: Run the FullRegistrationWorkflowTest from the command line. The test executable must exit with a code of 0, indicating all assertions passed.

Testing Tool: GTest, CTest.

User Story 2: Professional Documentation and User Guides

Description: As a new user, I need complete and easy-to-understand documentation so that I can quickly learn how to use the software effectively. This includes a getting-started guide for initial setup, detailed tutorials for key workflows, and a reference for all features.

Extremely Atomic Actions to Undertake:

Write User Guide Content:

Create a new directory: docs/user_guide/.

Create a new file: docs/user_guide/01_Getting_Started.md. Write sections for "System Requirements" and "Installation".

Create a new file: docs/user_guide/02_Your_First_Registration.md. Write a step-by-step tutorial that walks the user through the entire registration workflow using a sample dataset. Include screenshots for each major step.

Create a new file: docs/user_guide/03_Advanced_Features.md. Write sections explaining the difference between ICP variants and how to use the coordinate system transformation on export.

Write Technical and API Documentation:

Create a new directory: docs/technical/.

Create a new file: docs/technical/Architecture.md. Add a diagram and description of the MVP (Model-View-Presenter) architecture, showing how MainPresenter communicates with IMainView and the various backend services.

Use Doxygen to automatically generate HTML documentation from the source code comments. Configure the Doxyfile to scan all .h files in the src/ directory and output the result to docs/api/.

Create Training Materials:

Create a new directory: support/sample_projects/TutorialProject/.

Place two sample E57 scan files in this directory.

Add a README.md file inside TutorialProject/ that explicitly states the goal (e.g., "Register ScanB.e57 to ScanA.e57") and provides the expected final RMS error as a reference for the user.

Write Deployment and Maintenance Guides:

Create a new file: docs/administration/Deployment.md. Write sections for "Windows Deployment" (describing how to run the installer) and "Linux Deployment" (describing how to use the Docker container).

Create a new file: docs/administration/Maintenance.md. Write sections on "Backing Up Projects" and "Log File Location and Troubleshooting".

References between Files:

The user guide will reference the sample project in support/sample_projects/TutorialProject/.

The Doxygen configuration (Doxyfile) will reference the src/ directory.

Acceptance Criteria:

The User Guide contains clear, step-by-step instructions for completing a full registration.

The generated Doxygen API documentation includes all public classes and methods with their comment descriptions.

A new user can successfully complete the tutorial using the provided sample project and documentation.

Testing Plan:

Test Case 2.1: Documentation Usability Test.

Test Data: The TutorialProject dataset.

Expected Result: Provide the application and the 02_Your_First_Registration.md guide to a person who has never used the software. They must be able to complete the registration task successfully using only the documentation provided.

Testing Tool: Manual user testing.

User Story 3: Deployment Preparation and Production Readiness

Description: As a system administrator, I need a reliable and straightforward way to deploy the application. This includes a simple installer for Windows users and a containerized option for Linux to ensure the application runs consistently across different environments with all its dependencies included.

Extremely Atomic Actions to Undertake:

Create Windows Installer:

Create a new script file: deployment/installers/windows_installer.nsi (NSIS script).

Write the NSIS script to:

Define the application name and version.

Specify an output file name (e.g., CloudRegistration_v1.0_Installer.exe).

Package all required application files (the main .exe, all Qt .dlls, the shaders/ directory).

Create a Start Menu shortcut.

Create an uninstaller entry in "Add or Remove Programs".

Create Dockerized Deployment:

Create a new file: deployment/docker/Dockerfile.

In the Dockerfile, start from a base image (e.g., ubuntu:22.04).

Add commands to apt-get install all required runtime dependencies (like libgl1).

COPY the application executable and all necessary libraries from the build output into the container's /app directory.

Set the ENTRYPOINT to /app/CloudRegistration.

Implement Production Logging:

In src/main.cpp, modify the setupLogging() function.

Configure Qt's logging handler to write all qDebug(), qWarning(), and qCritical() messages to a file named CloudRegistration.log located in the application's user data directory (QStandardPaths::writableLocation(QStandardPaths::AppDataLocation)).

Implement log file rotation: if CloudRegistration.log exceeds 5MB, rename it to CloudRegistration.log.1 and start a new file.

Create Backup and Maintenance Scripts:

Create a new script: deployment/maintenance/backup_project.bat (for Windows).

This batch script should take a project path as an argument and create a .zip archive of that entire directory.

Create a corresponding backup_project.sh (for Linux).

References between Files:

The windows_installer.nsi script will reference the application executable produced by the CMake build process.

The Dockerfile will also reference the build output.

Acceptance Criteria:

The Windows installer successfully installs the application, and the application can be launched from the Start Menu.

The docker build and docker run commands execute successfully, and the application GUI launches from within the container.

After running the application, a CloudRegistration.log file is present in the specified directory and contains log messages.

The backup script successfully creates a zip archive of a specified project folder.

Testing Plan:

Test Case 3.1: Windows Installer Test.

Test Data: A clean Windows virtual machine.

Expected Result: Run the generated installer. Verify that the application installs, a Start Menu shortcut is created, and the program runs successfully. Then, use the uninstaller from "Add or Remove Programs" and verify all files are removed.

Testing Tool: Manual testing on a VM.

Conclusion
Sprint 8 is the final bridge between a feature-complete MVP and a trustworthy, professional product. By rigorously validating the system's stability, documenting its functionality, and creating a smooth deployment process, this sprint ensures that the application is not just powerful, but also reliable and accessible. Successful completion of this sprint signifies that the MVP is ready for its initial release to end-users.